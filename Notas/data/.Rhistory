consumerSecret <- "8rHDJKt8MUZ5t9XI0U85aqAoufBI7ZzTtjp8TI1bfZSaL3s1y2"
setup_twitter_oauth(consumerKey, consumerSecret)
rm(list=ls())
library(RCurl)
library(twitteR)
library(ROAuth)
library(httr)
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "http://api.twitter.com/oauth/access_token"
authURL <- "http://api.twitter.com/oauth/authorize"
consumerKey <- "hPPvJqM93Cd4orRU8nWAFSms7"
consumerSecret <- "8rHDJKt8MUZ5t9XI0U85aqAoufBI7ZzTtjp8TI1bfZSaL3s1y2"
setup_twitter_oauth(consumerKey, consumerSecret)
setup_twitter_oauth(consumerKey, consumerSecret)
rm(list=ls())
library(RCurl)
library(twitteR)
library(ROAuth)
library(httr)
consumerKey <- "hPPvJqM93Cd4orRU8nWAFSms7"
consumerSecret <- "8rHDJKt8MUZ5t9XI0U85aqAoufBI7ZzTtjp8TI1bfZSaL3s1y2"
setup_twitter_oauth(consumerKey, consumerSecret)
setup_twitter_oauth(consumerKey, consumerSecret,access_token=NULL, access_secret=NULL)
rm(list=ls())
require(RMySQL)
mydb = dbConnect(MySQL(), user=useR, password= passwoRd,
dbname='tutorial_database', host='localhost')
useR<-"root"
passwoRd<-"msckevin1064"
mydb = dbConnect(MySQL(), user=useR, password= passwoRd,
dbname='tutorial_database', host='localhost')
dbListTables(mydb)
dbListFields(mydb, 'EMP')
dbGetQuery(mydb, "select count(*) from EMP")
require(RPostgreSQL)
con <- dbConnect(PostgreSQL(), user= "postgres",
password="root", host='localhost', dbname='tutorial_database')
dbListTables(con)
dbListFields(con, 'emp')
dbGetQuery(mydb, "SELECT ENAME "NOMBRE DEL EMPLEADO" FROM EMP")
dbGetQuery(mydb, "SELECT ENAME 'NOMBRE DEL EMPLEADO' FROM EMP")
dbGetQuery(mydb, "SELECT JOB, ENAME FROM EMP ORDER BY JOB")
dbGetQuery(mydb, "SELECT ENAME FROM EMP ORDER BY JOB")
dbGetQuery(mydb, "SELECT DEPTNO, JOB FROM EMP ORDER BY 1")
dbGetQuery(mydb, "SELECT JOB, ENAME, SAL FROM EMP ORDER BY 1, 3 DESC")
dbGetQuery(mydb, "SELECT DISTINCT DEPTNO FROM EMP")
dbGetQuery(mydb, "SELECT ENAME, SAL * 4 * 12 'SAL ANUAL' FROM EMP")
dbGetQuery(mydb, "SELECT ENAME, SAL*COMM BONO FROM EMP")
dbGetQuery(mydb, "SELECT ENAME FROM EMP WHERE JOB = 'SALESMAN' ")
dbGetQuery(mydb, "SELECT ENAME FROM EMP WHERE DEPTNO <> 30")
dbGetQuery(con, "SELECT ENAME FROM EMP WHERE HIREDATE < '/1/1982'")
dbGetQuery(con, "SELECT ENAME FROM EMP WHERE HIREDATE < /1/1982")
dbGetQuery(con, "SELECT ENAME FROM EMP WHERE HIREDATE < '1981-01-01'")
db
dbGetQuery(con, "SELECT ENAME, HIREDATE FROM EMP WHERE JOB = 'ANALYST'")
dbGetQuery(con, "SELECT ENAME FROM EMP WHERE SAL > (SELECT SAL FROM EMP WHERE ENAME = 'ADAMS')")
db(con,"SELECT ENAME FROM EMP WHERE DEPTNO = (SELECT DEPTNO FROM EMP WHERE ENAME = 'CLARK')")
dbGetQuery(con, "SELECT ENAME FROM EMP WHERE DEPTNO = (SELECT DEPTNO FROM EMP WHERE ENAME = 'CLARK')")
library(twitteR)
library(tm)
library(wordcloud)
library(RColorBrewer)
mach_tweets <- searchTwitter("machine learning", n=500, lang="en")
mach_text <- sapply(mach_tweets, function(x) x$getText())
mach_corpus <- Corpus(VectorSource(mach_text))
mach_tweets <- searchTwitter("machine learning", n=500, lang="en")
library(twitteR)
library(tm)
library(wordcloud)
library(RColorBrewer)
setup_twitter_oauth(consumer_key = "hPPvJqM93Cd4orRU8nWAFSms7",
consumer_secret = "8rHDJKt8MUZ5t9XI0U85aqAoufBI7ZzTtjp8TI1bfZSaL3s1y2",
access_token = "295341155-CXBCjKvVu5KG5SJ4w2QMhJ1tPu7NeFjSjFIMb6Hu",
access_secret = "tpi5IKUouENVN7bfQoy8MsnvDgy3MO19i42Cbp8ssLQqp")
mach_tweets <- searchTwitter("machine learning", n=500, lang="en")
mach_text <- sapply(mach_tweets, function(x) x$getText())
mach_corpus <- Corpus(VectorSource(mach_text))
tdm <- TermDocumentMatrix(mach_corpus, control = list(removePunctuation = TRUE,
stopwords = c("machine", "learning", stopwords("english")),
removeNumbers = TRUE, tolower = TRUE))
m = as.matrix(tdm)
word_freqs = sort(rowSums(m), decreasing=TRUE)
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
mach_tweets <- searchTwitter("machine learning", n=1000, lang="en")
mach_text <- sapply(mach_tweets, function(x) x$getText())
mach_corpus <- Corpus(VectorSource(mach_text))
tdm <- TermDocumentMatrix(mach_corpus, control = list(removePunctuation = TRUE,
stopwords = c("machine", "learning", stopwords("english")),
removeNumbers = TRUE, tolower = TRUE))
m = as.matrix(tdm)
word_freqs = sort(rowSums(m), decreasing=TRUE)
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
mach_tweets <- searchTwitter("big data", n=1000, lang="en")
mach_text <- sapply(mach_tweets, function(x) x$getText())
mach_corpus <- Corpus(VectorSource(mach_text))
tdm <- TermDocumentMatrix(mach_corpus, control = list(removePunctuation = TRUE,
stopwords = c("big", "data", stopwords("english")),
removeNumbers = TRUE, tolower = TRUE))
m = as.matrix(tdm)
word_freqs = sort(rowSums(m), decreasing=TRUE)
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
mach_tweets <- searchTwitter("data mining", n=1000, lang="en")
mach_text <- sapply(mach_tweets, function(x) x$getText())
mach_corpus <- Corpus(VectorSource(mach_text))
tdm <- TermDocumentMatrix(mach_corpus, control = list(removePunctuation = TRUE,
stopwords = c("mining", "data", stopwords("english")),
removeNumbers = TRUE, tolower = TRUE))
m = as.matrix(tdm)
word_freqs = sort(rowSums(m), decreasing=TRUE)
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(9, "Dark2"))
mach_tweets <- searchTwitter("data mining", n=5000, lang="en")
mach_text <- sapply(mach_tweets, function(x) x$getText())
mach_corpus <- Corpus(VectorSource(mach_text))
tdm <- TermDocumentMatrix(mach_corpus, control = list(removePunctuation = TRUE,
stopwords = c("mining", "data", stopwords("english")),
removeNumbers = TRUE, tolower = TRUE))
m = as.matrix(tdm)
word_freqs = sort(rowSums(m), decreasing=TRUE)
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(10, "Dark2"))
library(ggplot2)
data(diamonds)
head(diamonds)
data(mtcars)
head(mtcars)
shiny::runApp('ShinyAppCoursera')
shiny::runApp('ShinyAppCoursera')
shiny::runApp('ShinyAppCoursera')
data(mtcars)
shiny::runApp('ShinyAppCoursera')
names(mtcars)
clusters <- kmeans(mtcars[,1],mtcars[,3])
mtcars[,1]
mtcars[,3]
clusters <- kmeans(mtcars[,c(1,3)],3)
class(clusters)
names(mtcars)
p<-ggplot(mtcars, aes(mpg, disp))+geom_point()
p<-ggplot(mtcars, aes(mpg, disp))
p+geom_point()
clusters$cluster
clusters$centers
p<-ggplot(mtcars, aes(mpg, disp))
p+geom_point(colors=clusters$centers)
dat<-as.data.frame(clusters$centers)
dat
p<-ggplot(mtcars, aes(mpg, disp))+geom_point()
p+geom_point(data=dat,aes(mpg, disp))
p<-ggplot(mtcars, aes(mpg, disp))+geom_point()
p+geom_point(data=dat,aes(mpg, disp), colour="red", size = 4)
class(clusters$cluster)
class(clusters$cluster)
shiny::runApp('ShinyAppCoursera')
shiny::runApp('ShinyAppCoursera')
shiny::runApp('ShinyAppCoursera')
shiny::runApp('ShinyAppCoursera')
shiny::runApp('ShinyAppCoursera')
shiny::runApp('ShinyAppCoursera')
shiny::runApp('ShinyAppCoursera')
shinyapps::setAccountInfo(name='klperez',
token='3C5AE8E3A704F8C62A22DD86F4AE52AC',
secret='<SECRET>')
library("shinyapps", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.1")
shinyapps::setAccountInfo(name='klperez',
token='3C5AE8E3A704F8C62A22DD86F4AE52AC',
secret='<SECRET>')
library("shinyapps", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.1")
shinyapps::setAccountInfo(name='klperez', token='3C5AE8E3A704F8C62A22DD86F4AE52AC', secret='0lp5gkFxtZX+r4WFiJjvqvvZwVZUJ+zQuSkWgX/Z')
deployApp("/home/kevin/ShinyAppCoursera")
deployApp("/home/kevin/ShinyAppCoursera")
shinyapps::setAccountInfo(name='klperez', token='3C5AE8E3A704F8C62A22DD86F4AE52AC', secret='0lp5gkFxtZX+r4WFiJjvqvvZwVZUJ+zQuSkWgX/Z')
deployApp("/home/kevin/ShinyAppCoursera")
install.packages("TeachingSampling")
require(TeachingSampling)
A <- c(0.30, 0.20, 0.40, 0.05, 0.00, 0.05)
B<-c(0.40, 0.10, 0.30, 0.00, 0.20, 0.00)
A*B
sum(A*B)
A <- c(0.10, 0.50, 0.00, 0.40, 0.00, 0.00)
B<-c(0.20, 0.40, 0.30, 0.00, 0.10, 0.00)
sum(A*B)
pbinom(q = ,size = , )
pbinom(q = ,size = ,prob = ,  )
dbinom(x = 2, size = 10, prob = 0.3)
me = (1-1/1000000)/(1/1000000)
me
p=1/1000000
(1-p)/p
dpois(x = 0,lambda = 2)
ppois(q = 2, lambda = 2)
ppois(q = 2, lambda = 2, lower.tail = T)
ppois(q = 2, lambda = 2, lower.tail = F)
ppois(q = 2, lambda = 2, lower.tail = F)
dnbinom(x = 2, size = 5, prob = 0.5)
dnbinom(x = 2, size = 3, prob = 0.5)
dnbinom(x = 5, size = 2, prob = 0.5)
dnbinom(x = 2, size = 3,prob = 0.5)
dnbinom(x = 5, size = 3,prob = 0.5)
dnbinom(x = 5, size = 2,prob = 0.5)
dnbinom(x = 5, size = 2,prob = 1/2)
dnbinom(x = 3, size = 2,prob = 1/2)
dnbinom(x = 3, size = 5,prob = 1/2)
dnbinom(x = 2, size = 5,prob = 1/2)
dnbinom(x = 2, size = 3,prob = 1/2)
dnbinom(x = 2, size = 3,prob = 1/2)
dnbinom(x = 3, size = 2,prob = 1/2)
dnbinom(x = 2, size = 3,prob = 1/2)
dnbinom(x = 2, size = 5, prob = 1/2)
dnbinom(x = 5, size = 2, prob = 1/2)
runif(10, 15, 20)
pexp(q = 1, rate = 1/15, lower.tail = F)
pexp(q = 1, rate = 1/15, lower.tail = T)
pexp(q = 1, rate = 1/5, lower.tail = )
pexp(q = 60, rate = 1/5, lower.tail = T)
pexp(q = 60, rate = 1/5, lower.tail = F)
data <- read.csv(file.choose(), header=T)
head(data)
tail(data)
summary(data)
getwd()
getwd()
setwd("/home/kevin/Documentos/Data Visualization/Programming Assignment 1 Data New")
summary(data)
data <- read.csv("/home/kevin/ExcelFormattedGISTEMPData2CSV.csv", header=T)
p <- ggplot(data, aes(x=Year, y=Glob))
library(ggplot2)
data <- read.csv("/home/kevin/ExcelFormattedGISTEMPData2CSV.csv", header=T)
p <- ggplot(data, aes(x=Year, y=Glob))
p + geom_line()
p + geom_line(aes(colour =  NHem )) + scale_colour_gradient(low="red")
names(data)
p <- ggplot(data, aes(x=Year))
p + geom_line(aes(y =  NHem )) + geom_line(aes(y =  SHem ))
p <- ggplot(data, aes(x=Year))
p + geom_line(aes(y =  NHem , colour="red")) + geom_line(aes(y =  SHem, colour="blue" ))
p + geom_line(aes(y =  NHem , col="red")) + geom_line(aes(y =  SHem, colour="blue" ))
p + geom_line(aes(y =  NHem , col="red")) + geom_line(aes(y =  SHem, col="blue" ))
p <- ggplot(data, aes(x=Year))
p + geom_line(aes(y =  NHem , col=NHem)) + geom_line(aes(y =  SHem, col=SHem ))
p <- ggplot(data, aes(x=Year))
p + geom_line(aes(y =  NHem , col='NHem')) + geom_line(aes(y =  SHem, col='SHem' ))
melt(data, id=Year)
library("reshape", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.1")
library("reshape2", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.1")
melt(data, id=Year)
melt(data, id='Year')
data1<-melt(data, id='Year')
head(data1)
data1<-melt(data[, 1:3], id='Year')
head(data1)
p<-ggplot(data1, aes(x = Year, y = value, colour = variable)) +
geom_line()
p
data[, 1:3]
data1<-melt(data[, 1:4], id='Year')
p<-ggplot(data1, aes(x = Year, y = value, colour = variable)) +
geom_line()
p
p<-ggplot(data1, aes(x = Year, y = value, colour = variable)) + geom_line()
p + ylab(label="Value of temperature")
p <- ggplot(data, aes(x=Year))
p + geom_line(aes(y =  NHem , col='NHem')) + geom_line(aes(y =  SHem, col='SHem' ))
p <- ggplot(data, aes(x=Year))
p + geom_line(aes(colour = Glob))
p <- ggplot(data, aes(x=Year, y =Glob))
p + geom_line(aes(colour = Glob))
names(data)
p <- ggplot(data, aes(x=Year, y =Glob))
p + geom_line(aes(colour = X24N.90N))
p <- ggplot(data, aes(x=Year, y =Glob))
p + geom_line(aes(colour = X24N.90N))+ scale_colour_gradient(low="red")
library("igraph", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.1")
library(igraph)
# Make up data
relations <- data.frame(from=c("Bob", "Cecil", "Cecil", "David", "David", "Esmeralda"),
to=c("Alice", "Bob", "Alice", "Alice", "Bob", "Alice"),
weight=c(4,5,5,2,1,1))
# Alternatively, you could read in the data from a similar CSV file as follows:
# relations <- read.csv("relations.csv")
# Load (DIRECTED) graph from data frame
g <- graph.data.frame(relations, directed=TRUE)
# Plot graph
plot(g, edge.width=E(g)$weight)
bsk<-read.table("http://www.dimiter.eu/Data_files/edgesdata3.txt",
sep='\t', dec=',', header=T)
bsk.network<-graph.data.frame(bsk, directed=F)
plot(bsk.network)
dhyper(x = 2, m = 4, n = 10,k = 3)
dhyper(x = 2, m = 3, n = 10,k = 4)
dhyper(x = 2, m = 3, n = 10,k = 4)
dpois(x = 5, lambda = 10)
dpois(x = 3, lambda = 2)
ppois(q = 4, lambda = 5.2, lower.tail = F)
ppois(q = 7, lambda = 7.8, lower.tail = F)
ppois(q = 7, lambda = 7.8, lower.tail = T)
1 - ppois(q = 7, lambda = 7.8, lower.tail = T)
pexp(q = 5, rate = 1/12, lower.tail = F)
pexp(q = 5, rate = 1/12, lower.tail = T)
pexp(q = 5, rate = 12, lower.tail = F)
pexp(q = 5, rate = 12, lower.tail = T)
pexp(q = 5, rate = 1/12, lower.tail = F)
pexp(q = 5, rate = 1/12, lower.tail = T)
dexp(q = 5, rate = 1/12, lower.tail = T)
dexp(x = 5, rate = 1/12)
pexp(q = 5, rate = 1/12, lower.tail = F)
punif(q = 25, min = 23.8, max = 26.2, lower.tail = F)
punif(q = 2.4, min = 23.8, max = 26.2, lower.tail = F)
pnorm(q = 63, mean = 67, sd = 2, lower.tail = F)
pnorm(q = 63, mean = 67, sd = 2, lower.tail = T)
dnorm(x = 63, mean = 67, sd = 2)
dnorm(x = 70.5, mean = 67, sd = 2)
pnorm(q = 70.5, mean = 67, sd = 2, lower.tail = T)
pnorm(q = 70.5, mean = 67, sd = 2, lower.tail = F)
1-pnorm(q = 70.5, mean = 67, sd = 2, lower.tail = F)
pnorm(q = 70.5, mean = 67, sd = 2, lower.tail = F)
pnorm(q = 69.3, mean = 67, sd = 2, lower.tail = F)
pnorm(q = 69.3, mean = 0, sd = 1, lower.tail = F)
pnorm(q = 1, mean = 0, sd = 1, lower.tail = F)
pnorm(q = -1.25, mean = 0, sd = 1, lower.tail = F)
pnorm(q = -1.25, mean = 0, sd = 1, lower.tail = T)
pnorm(q = 1.65, mean = 0, sd = 1, lower.tail = T)
pnorm(q = 1.65, mean = 0, sd = 1, lower.tail = F)
sample(c("H","T"), size = 1000, replace =TRUE)
a<-sample(c("H","T"), size = 1000, replace =TRUE)
str(a)
dhyper(5, m = 17, n = 233, k = 5)
phyper(2, m = 17, n = 233, k = 5)
phyper(1, m = 17, n = 233, k = 5, lower.tail = FALSE)
pgeom(4, prob = 0.812, lower.tail = FALSE)
dnbinom(5, size = 7, prob = 0.5)
diff(ppois(c(47, 50), lambda = 50))
f <- function(x) 3 * x^2
integrate(f, lower = 0.14, upper = 0.71)
pnorm(q = 64.5, mean = 67, sd = 4, lower.tail = F)
pnorm(q = 64.5, mean = 67, sd = 4, lower.tail = T)
pnorm(-1.25, 0, 1)
pnorm(-1.25, lower.tail = F)
pnorm(-1.25, lower.tail = T)
pnorm(70.3, 67, 2)
pnorm(70.3, 67, 2,lower.tail = F)
pnorm(70.3, 67, 4,lower.tail = F)
1-pnorm(70.3, 67, 2)
pnorm(70.3, 67, 1,lower.tail = F)
pnorm(70.3, 67, 2,lower.tail = F)
pnorm(q = 70.3, mean = 67, sd = 2,lower.tail = F)
pnorm(84, mean=72, sd=15.2, lower.tail=FALSE)
pnorm(q = 64.5, mean = 67, sd = 2,lower.tail = T)
pnorm(q = 1.97)
1-pnorm(q = 1.97)
1-pnorm(q = 1.65)
1-pnorm(q = -1.25)
pnorm(2)
pnorm(2, lower.tail = F)
qnorm(2, lower.tail = F)
qnorm(p = 0.02)
qnorm(p = 0.02,lower.tail = F)
pnorm(1.86)
pnorm(1.86, lower.tail = T)
pnorm(1.86, lower.tail = F)
pnorm(1.86, lower.tail = F)*2
pnorm(-1.78, lower.tail = F)*2
pnorm(-1.78, lower.tail = F)
pnorm(-1.78, lower.tail = T)
pnorm(-1.78, lower.tail = T)*2
pnorm(1.78, lower.tail = F)*2
pnorm(1.93)
pnorm(1.93, lower.tail = F)
qnorm(0.99)
qnorm(0.99, lower.tail = F)
qnorm(0.99, lower.tail = T)
qnorm(0.01, lower.tail = T)
qnorm(0.01, lower.tail = F)
qnorm((1-0.99)/2, lower.tail = T)
qnorm((1-0.99)/2, lower.tail = T)
qnorm(1-(0.99/2), lower.tail = T)
qnorm(1-(0.99/2), lower.tail = F)
0.99/2
1-0.495
qnorm(0.505, lower.tail = F)
qnorm(1-(0.01/2), lower.tail = F)
Blogs <- readLines("/home/kevin/Documentos/final/final/en_US/en_US.blogs.txt")
News <- readLines("/home/kevin/Documentos/final/final/en_US/en_US.news.txt")
Twitter <- readLines("/home/kevin/Documentos/final/final/en_US/en_US.twitter.txt")
Sam <- length(News) * 0.05
Corpus <- sample(News, Sam)
MyCorpus <- Corpus(VectorSource(Corpus))
library(tm)
set.seed(555)
Sam <- length(News) * 0.05
Corpus <- sample(News, Sam)
MyCorpus <- Corpus(VectorSource(Corpus))
MyCorpus <- tm_map(MyCorpus, content_transformer(iconv), from = "latin1", to = "ASCII", sub = "")
MyCorpus <- tm_map(MyCorpus, content_transformer(tolower))
MyCorpus <- tm_map(MyCorpus, removePunctuation)
MyCorpus <- tm_map(MyCorpus, stripWhitespace)
MyCorpus <- tm_map(MyCorpus, removeNumbers)
MyCorpus <- tm_map(MyCorpus, removeWords, stopwords("english"))
inspect(MyCorpus[1:2])
MyDtm <- TermDocumentMatrix(MyCorpus)
findAssocs(dtm2, " travel ", 0.15)
findAssocs(MyDtm, " travel ", 0.15)
findAssocs(MyDtm, " travel ")
findAssocs(MyDtm, "travel", .001)
findAssocs(MyDtm, "travel", .01)
findAssocs(MyDtm, "travel", .1)
findAssocs(MyDtm, "travel", .5)
dat<-c(19,21,20,20,19,19,20,23)
mean(dat)
fileUrl<-"http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc<-xmlTreeParse(fileUrl,useInternal=TRUE)
library(XML)
fileUrl<-"http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc<-xmlTreeParse(fileUrl,useInternal=TRUE)
rootNode<-xmlRoot(doc)
xmlName(rootNode)
Y<-xpathSApply(rootNode,"//zipcode",xmlValue)
Y
table(Y%in%c("21231"))
class(Y)
Url<-"http://biostat.jhsph.edu/~jleek/contact.html"
doc<-htmlTreeParse(Url, useInternalNodes = T)
doc
rootNode<-xmlRoot(doc)
xmlName(rootNode)
nchar(x = Doc)
class(doc)
library(swirl)
swirl()
getwd()
author("DataTable")
library(slidify)
library(slidifyLibraries)
author("DataTable")
getwd()
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
slidify("index.Rmd")
library(swirl)
swirl()
devtools::install_github(c("swirldev/swirl", "swirldev/swirlify"))
swirl()
En el conjunto de datos proporcionado para este concurso, ¿cuáles son los nombres de las columnas del conjunto de datos?
getwd()
setwd("/home/kevin/Documents")
setwd("/home/kevin/Documents/")
setwd("/home/kevin/Documentos/Clases Unicor/Notas/data/")
setwd("/home/kevin/Documentos/Clases Unicor/Data Mining/Notas/data/")
if(!file.exists("./data")){dir.create("./data")}
fileUrl <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl,destfile="./data/USC.csv")
UscData <- read.csv("./data/USC.csv")
class(UscData$VAL)
UscData$VAL
sum(UscData$VAL==24)
sum(UscData$VAL=='24')
sum(UscData$VAL)
UscData$VAL==24
UscData$VAL
sum(UscData$VAL==24)
sum(UscData$VAL==24, na.rm = T)
Property<-sum(UscData$VAL==24, na.rm = T)
Property
library(XML)
fileUrl<-"http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc<-xmlTreeParse(fileUrl,useInternal=TRUE)
rootNode<-xmlRoot(doc)
xmlName(rootNode)
Y<-xpathSApply(rootNode,"//li[@class='zipcode']",xmlValue)
Y
table(Y%in%c("21231"))
Y<-xpathSApply(rootNode,"/li[@class='zipcode']",xmlValue)
Y
Y<-xpathSApply(rootNode,"//li[@name='zipcode']",xmlValue)
Y
table(Y%in%c("21231"))
Y<-xpathSApply(rootNode,"//zipcode",xmlValue)
Y
zipcode<-xpathSApply(rootNode,"//zipcode",xmlValue)
table(zipcode%in%c("21231"))
sum(zipcode=="21231")
Url<-"http://biostat.jhsph.edu/~jleek/contact.html"
doc<-htmlTreeParse(Url, useInternalNodes = T)
rootNode<-xmlRoot(doc)
xmlName(rootNode)
doc
class(doc)
doc<-htmlTreeParse(Url, useInternal = T)
doc
class(doc)
Url<-"http://biostat.jhsph.edu/~jleek/contact.html"
readLines(con = Url, n = 100)
doc<-readLines(con = Url, n = 100)
calss(doc)
class(doc)
doc
nchar(doc[c(10,20,30,100)])
fUrl<-"https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
docx<-read.fwf(file = fUrl)
docx<-read.fwf(file = fUrl, widths = c(12, 7,4, 9,4, 9,4, 9,4))
docx<-read.fwf(file = fUrl, widths = 12)
docx<-read.fwf(file=url("http://www.cpc.ncep.noaa.gov/data/indices/wksst8110.for"),
skip=4, widths=c(12, 7,4, 9,4, 9,4, 9,4)))
head (x)
fUrl<-url("http://www.cpc.ncep.noaa.gov/data/indices/wksst8110.for")
Doc<-read.fwf(file=fUrl, skip=4, widths=c(12, 7,4, 9,4, 9,4, 9,4)))
Doc<-read.fwf(file=fUrl, skip=4, widths=c(12, 7,4, 9,4, 9,4, 9,4))
head(Doc)
